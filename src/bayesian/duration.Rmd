---
title: "Requirements Quality Impact on Duration"
author: "Julian Frattini"
date: '2023-10-11'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

source("../util/setup.R")
source("../util/model-eval.R")
```


This document contains the investigation of the impact of **requirements quality** on domain modeling **duration**, i.e., the time in minutes it took a experiment participant to generate a domain model from a requirements specification.

## Data Loading

Load the data and ensure that all variables are properly set up.

```{r data}
source("../util/data-preprocessing.R")
d <- load.data()

str(d)
```

## Bayesian Data Analysis 

We perform a Bayesian data analysis to answer the question: which factors influence the amount of time it takes to derive the domain model from the requirements specification.

### Graphical Causal Model

We draw a Directed Acyclic Graph (DAG) to make our causal assumptions explicit.

```{r dag}
dag <- dagify(
  dur ~ rq + edu + exp + dom + task + tool,
  task ~ exp,
  dom ~ exp,
  exposure = "rq",
  outcome = "dur",
  labels = c(dur = "duration", rq="requirements.quality",
             edu = "education", exp="experience",
             dom = "domain.knowledge", task = "task.experience",
             tool = "tool.experience"),
  coords = list(x=c(rq=1, dur=2, edu=1, exp=1, dom=1.2, task=1, tool=1),
                y=c(rq=0, dur=-1.5, edu=-2, exp=-3.5, dom=-4, task=-3, tool=-2.5))
)

ggdag_status(dag, 
             use_labels = "label", 
             text = FALSE) +
  guides(fill = "none", color = "none") + 
  theme_dag()
```

### Formula Definition and Prior Selection

We derive our formula from our DAG to infer the strength of causal relationships from it.

#### Model Definition

Based on the DAG, we construct a model with the following parameters:

| Term | Predictor | Rationale |
|---|---|---|
| `0` | Intercept | Disabling the intercept because the data is centered around 0 |
| `(1|PID)` | Participant-specific variability | Variation in the duration caused by the individual experiment participants |
| `RQ*period` | Carryover | Interaction effect between the treatment and the period, i.e., whether some treatments have a stronger effect in later periods |
| (`RQ`) |  Main Factor | Influence of a requirements quality defect on connecting entities wrongly |
| (`period`) | Sequence effect | Period in which the subject applies the treatment |
| *rest* | Confounders | Context factors influencing the causal relationships. |

Predictors in brackets (e.g., (`RQ`)) are implied by the interaction effects (e.g., `RQ*period`) and do not explicitly appear in the formula.

```{r formula}
formula <- rel.duration ~ 0 + 
               (1|PID)+ RQ*period +
               exp.se.scaled + exp.re.scaled + edu + primary.role + 
               dom.os + dom.db + 
               model.train + model.occ 

get_prior(formula, data=d, family=gaussian)
```

For each predictor, we select an appropriate prior that reflects our prior belief about the causal relationship. We use weakly informative priors to allow the model in the later estimation step to infer the strength of the causal relations itself.

```{r priors}
priors <- c(
  prior(normal(0, 0.3), class=b),
  prior(weibull(2,1), class=sigma),
  prior(weibull(2,1), class=sd)
)
```

#### Prior Predictive Checks

We consider two response variable distributions: normal and skewed-normal distribution.

```{r ndraws}
ndraws <- 100
```

We sample `ndraws=100` draws directly from the priors (i.e., sample without the model updating its parameter distributions based on the observed data) to evaluate the feasibility of the priors. 

```{r model-priors-normal}
m.prior.norm <-
  brm(data = d, family = gaussian, formula, prior = priors,
    iter = 4000, warmup = 1000, chains = 4, cores = 4,
    seed = 4, sample_prior="only",
    file = "fits/duration.norm.prior"
  )
```

```{r model-priors-skewnormal}
m.prior.skew <-
  brm(data = d, family = skew_normal, formula, prior = priors,
    iter = 4000, warmup = 1000, chains = 4, cores = 4,
    seed = 4, sample_prior="only",
    file = "fits/duration.skew.prior"
  )
```

With the samples from the prior distributions, we conduct a prior predictive check to ensure that the priors are appropriate, i.e., the actual observations are realistic given the prior knowledge of the model.

```{r prior-predictive-check}
priorpc.norm <- brms::pp_check(m.prior.norm, ndraws=ndraws)
priorpc.skew <- brms::pp_check(m.prior.skew, ndraws=ndraws)

priorpc.norm | priorpc.skew
```

The distribution of draws encompasses the actually observed data, meaning that the actual observations lie within the realm of belief defined by the priors. We accept our priors as feasible.

### Model Training

Now, we execute the estimation step by updating the prior distributions based on the observed data `d`.

```{r model}
m.norm <-
  brm(data = d, family = gaussian, formula, prior = priors,
    iter = 4000, warmup = 1000, chains = 4, cores = 4,
    seed = 4, 
    file = "fits/duration.norm"
  )
```

```{r model-skew}
m.skew <-
  brm(data = d, family = skew_normal, formula, prior = priors,
    iter = 4000, warmup = 1000, chains = 4, cores = 4,
    seed = 4, 
    file = "fits/duration.skew"
  )
```

We perform a posterior predictive check to ensure that the model has learned properly.

```{r posterior-predictive-check}
postpc.norm <- brms::pp_check(m.norm, ndraws=ndraws)
postpc.skew <- brms::pp_check(m.skew, ndraws=ndraws)

postpc.norm | postpc.skew
```

The distribution of draws still encompasses the actually observed data. Additionally, the distribution grew narrower around the observed data, indicating that the posterior distributions more accurately reflect the the observed data. However, the model does not fully reflect the form of the actual data, which hints at the necessity to rethink the chosen distribution type of the response variable.

### Comparison regarding predictive Power

We compare the predictive capability of the two models using the leave-one-out comparison.

```{r loo-compare}
m.norm <- add_criterion(m.norm, criterion = "loo")
m.skew <- add_criterion(m.skew, criterion = "loo")

loo_compare(m.norm, m.skew)
```

The model using the skewed normal distribution outperforms the other model in terms of predictive power according to loo_compare.

### Comparison regarding conditional Independencies

Additionally, we test for conditional independencies. According to our DAG, we assume that $exp.se \rightarrow duration$, $exp.se \rightarrow dom$, and $dom \rightarrow duration$. We test whether the first and/or last hypothesis are true by comparing the selected model with other candidates where one of the two factors is missing. 

```{r comparison-formula}
f.no.exp <- rel.duration ~ 0 + 
               (1|PID)+ RQ*period +
               exp.re.scaled + edu + primary.role + 
               dom.os + dom.db + 
               model.train + model.occ 

f.no.dom <- rel.duration ~ 0 + 
               (1|PID)+ RQ*period +
               exp.se.scaled + exp.re.scaled + edu + primary.role +
               model.train + model.occ 
```

```{r model-skew-comp}
m.skew.no.exp <-
  brm(data = d, family = skew_normal, f.no.exp, prior = priors,
    iter = 4000, warmup = 1000, chains = 4, cores = 4,
    seed = 4
  )

m.skew.no.dom <-
  brm(data = d, family = skew_normal, f.no.dom, prior = priors,
    iter = 4000, warmup = 1000, chains = 4, cores = 4,
    seed = 4
  )
```

We compare the predictors of interest ($\beta_{exp.se\_scaled}$ and $\beta_{dom.os}$) to see whether the estimates change when including either of the variables.

```{r conditional-independencies}
param.m.skew <- mcmc_intervals_data(m.skew)
param.m.skew.no.exp <- mcmc_intervals_data(m.skew.no.exp)
param.m.skew.no.dom <- mcmc_intervals_data(m.skew.no.dom)

param.m.skew$model <- "full model"
param.m.skew.no.exp$model <- "no exp.se"
param.m.skew.no.dom$model <- "no dom"

combined.estimates <- rbind(param.m.skew, param.m.skew.no.exp, param.m.skew.no.dom) %>% 
  filter(parameter %in% c("b_exp.se.scaled", 
                          "b_dom.os.L", "b_dom.os.Q", "b_dom.os.C", "b_dom.os.E4", 
                          "b_dom.db.L", "b_dom.db.Q", "b_dom.db.C"))

position.offset <- position_nudge(
  y = ifelse(
    combined.estimates$model == "full model",
    -0.1,
    ifelse(combined.estimates$model == "no dom", 0.1, 0)))

ggplot(data = combined.estimates,
       mapping = aes(x = m, y = parameter, color = model)) +
  geom_linerange(aes(xmin = l, xmax = h), position = position.offset, size = 2) +
  geom_linerange(aes(xmin = ll, xmax = hh), position = position.offset) +
  geom_point(position = position.offset, color="black") + 
  geom_vline(xintercept = 0, linetype="dashed")
```

It becomes apparent that none of the variables are independent of each other: learning either variable does not diminish the information gained from the other variables. Consequently, we assume all three causal assumptions being correct.

### Model selection

We select `m.skew` as the model to evaluate given that (1) it has the most predictive power and (2) none of the causal assumptions are violated.

```{r model-selection}
m <- m.skew
```

We visualize the summary of updated posterior distributions - which does not yet answer the initial question, but indicates the direction and strength that each predictor has on the response variable.

```{r model-summary}
summary(m)
```

### Model Evaluation

With the best causal model selected, we evaluate it by sampling from the posterior and plotting some marginal effects.

#### Posterior Predictions

We let the model predict the response variable with a fixed independent variable (RQ=1 (passive voice), RQ=2 (ambiguous pronoun), and RQ=3 (passive voice + ambiguous pronoun)) and representative values for the context factors. This posterior prediction takes all uncertainty, which the model picked up, into account.

```{r posterior-comparison-passive}
evaluate.model(model=m, treatment=1)
```

```{r posterior-comparison-pronoun}
evaluate.model(model=m, treatment=2)
```

```{r posterior-comparison-passive-pronoun}
evaluate.model(model=m, treatment=3)
```

For both independent variables, the posterior predictions do not lean strongly to either of the two signs. We conclude that the effect of passive voice and ambiguous pronouns on the duration of domain modeling is negligible.

These values can also be obtained via `brms::conditional_effects`: by selecting the treatment `RQ` as the effect, the `posterior_epred` method predicts the distribution of the expected value when fixing the treatment to one of the four available factors while maintaining all other independent variables at representative values.

```{r posterior-comparison}
conditional_effects(m, effects="RQ", method="posterior_epred")
```

#### Marginal and Interaction Effects

Finally, we plot marginal and interaction effects of interesting context factors to see their isolated impact. For example, the marginal effect of the `period` variable shows a slight learning effect. The conditional effect between the `period` variable and the treatment `RQ` shows that this effect is fairly comparable for most treatments except `RQ=1`. This implies a slight carryover-effect, i.e., the effect of a treatment slightly depends on the period in which it is applied. In this case, the later pure passive voice is applied to the requirements specification, the stronger its (negative) effect on the relative duration.

```{r marginal-training}
conditional_effects(m, effects=c("period", "period:RQ"))
```

None of the other context factors have a major influence on the response variable according to the summary.

```{r}
conditional_effects(m, effects="dom.os")
```

