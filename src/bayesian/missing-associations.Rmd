---
title: "Requirements Quality Impact on Missing Associations"
author: "Julian Frattini"
date: '2023-10-11'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

source("../util/setup.R")
source("../util/model-eval.R")
```

This document contains the investigation of the impact of **requirements quality** on the number of **missing associations** in the process of domain modeling. An association is missing if it is contained in the gold standard (i.e., how the domain model *should* look like when derived correctly from a requirements specification) but not in the domain model delivered by an experiment participant.

## Data Loading

```{r data}
source("../util/data-preprocessing.R")
d <- load.data()
```

## Bayesian Data Analysis 

We perform a Bayesian data analysis to answer the question: which factors influence the number of missing associations in a domain model.

### Graphical Causal Model

We draw a Directed Acyclic Graph (DAG) to make our causal assumptions explicit.

```{r dag}
dag <- dagify(
  ma ~ rq + me + dur + edu + exp + dom + task + tool,
  me ~ rq,
  dom ~ exp,
  exposure = "rq",
  outcome = "ma",
  labels = c(ma = "missing.association", me="missing.entities",
             dur = "duration",
             rq = "requirements.quality",
             edu = "education", exp="experience",
             dom = "domain.knowledge", task = "task.experience",
             tool = "tool.experience"),
  coords = list(x=c(rq=1, ma=2, me=1.5, dur=1.5, edu=1, exp=1, dom=1.2, task=1, tool=1),
                y=c(rq=-1, ma=-1.5, me=-0.5, dur=0.5, edu=-2, exp=-3.5, dom=-4, task=-2.5, tool=-3))
)

ggdag_status(dag, 
             use_labels = "label", 
             text = FALSE) +
  guides(fill = "none", color = "none") + 
  theme_dag()
```

We do not assume a relationship between *requirements quality* and *duration* as we have already disproven this relationship in the [duration](./duration.Rmd) analysis.

### Formula Definition and Prior Selection

We derive our formula from our DAG to infer the strength of causal relationships from it.

#### Model Definition

Based on the DAG, we construct a model with the following parameters: (see `wrong-associations.Rmd` for details)

```{r formula}
formula <- associations.missing | trials(associations.expected) ~ 1 + 
  (1|PID) + 
  RQ*period +
  entities.missing +
  rel.duration + 
  exp.se.scaled + exp.re.scaled + edu + primary.role + 
  model.train + model.occ +
  dom.os + dom.db
```

For each predictor, we select an appropriate prior that reflects our prior belief about the causal relationship. We use weakly informative priors to allow the model in the later estimation step to infer the strength of the causal relations itself.

```{r priors}
priors <- c(
  prior(normal(-0.9, 0.1), class=Intercept),
  prior(normal(0, 0.5), class=b, coef="RQ1"),
  prior(normal(0, 0.5), class=b, coef="RQ2"),
  prior(normal(0, 0.5), class=b, coef="RQ3"),
  prior(normal(0, 0.1), class=b),
  prior(weibull(2, 0.5), class=sd)
)
```

#### Prior Predictive Checks

We sample directly from the priors (i.e., sample without the model updating its parameter distributions based on the observed data) to evaluate the feasibility of the priors.

```{r model-sample-priors}
m.prior <-
  brm(data = d, family = binomial, formula, prior = priors,
    iter = 4000, warmup = 1000, chains = 4, cores = 4,
    seed = 4, sample_prior="only",
    file = "fits/missing.associations.prior"
  )
```

With the samples from the prior distributions, we conduct a prior predictive check to ensure that the priors are appropriate, i.e., the actual observations are realistic given the prior knowledge of the model.

```{r prior-predictive-check}
ndraws <- 100
priorpc <- brms::pp_check(type="bars", m.prior, ndraws=ndraws)
priorpc
```

The distribution of draws encompasses the actually observed data, meaning that the actual observations lie within the realm of belief defined by the priors. We accept our priors as feasible.

### Model Training

Now, we execute the estimation step by updating the prior distributions based on the observed data `d`.

```{r model}
m <-
  brm(data = d, family = binomial, formula, prior = priors,
    iter = 4000, warmup = 1000, chains = 4, cores = 4,
    seed = 4, 
    file = "fits/missing.associations"
  )
```

We perform a posterior predictive check to ensure that the model has learned properly.

```{r posterior-predictive-check}
brms::pp_check(m, type="bars", ndraws=ndraws)
```

The distribution of draws still encompasses the actually observed data. However, the distributions have not become significantly narrower around their mean, which leaves room for improvement.

Because we only trained one model, there is no need to select the best performing model using leave-one-out comparison `loo_compare`.

### Comparison regarding conditional Independencies

We test for conditional independencies. 

#### Experience and Domain Knowledge

According to our DAG, we assume that $exp.se \rightarrow missing.associations$, $exp.se \rightarrow dom$, and $dom \rightarrow missing.associations$. We test whether the first and/or last hypothesis are true by comparing the selected model with other candidates where one of the two factors is missing. 

```{r comparison-formula}
f.no.exp <- associations.missing | trials(associations.expected) ~ 1 + 
  (1|PID) + 
  RQ*period +
  entities.missing +
  rel.duration + 
  exp.re.scaled + edu + primary.role + 
  model.train + model.occ +
  dom.os + dom.db

f.no.dom <- associations.missing | trials(associations.expected) ~ 1 + 
  (1|PID) + 
  RQ*period +
  entities.missing +
  rel.duration + 
  exp.se.scaled + exp.re.scaled + edu + primary.role + 
  model.train + model.occ
```

```{r model-skew-comp}
m.no.exp <-
  brm(data = d, family = binomial, f.no.exp, prior = priors,
    iter = 4000, warmup = 1000, chains = 4, cores = 4,
    seed = 4, 
  )

m.no.dom <-
  brm(data = d, family = binomial, f.no.dom, prior = priors,
    iter = 4000, warmup = 1000, chains = 4, cores = 4,
    seed = 4
  )
```

We compare the predictors of interest ($\beta_{exp.se\_scaled}$ and $\beta_{dom.os}$) to see whether the estimates change when including either of the variables.

```{r conditional-independencies}
param.m <- mcmc_intervals_data(m)
param.m.no.exp <- mcmc_intervals_data(m.no.exp)
param.m.no.dom <- mcmc_intervals_data(m.no.dom)

param.m$model <- "full model"
param.m.no.exp$model <- "no exp.se"
param.m.no.dom$model <- "no dom"

combined.estimates <- rbind(param.m, param.m.no.exp, param.m.no.dom) %>% 
  filter(parameter %in% c("b_exp.se.scaled", 
                          "b_dom.os.L", "b_dom.os.Q", "b_dom.os.C", "b_dom.os.E4", 
                          "b_dom.db.L", "b_dom.db.Q", "b_dom.db.C"))

position.offset <- position_nudge(
  y = ifelse(
    combined.estimates$model == "full model",
    -0.1,
    ifelse(combined.estimates$model == "no dom", 0.1, 0)))

ggplot(data = combined.estimates,
       mapping = aes(x = m, y = parameter, color = model)) +
  geom_linerange(aes(xmin = l, xmax = h), position = position.offset, linewidth = 2) +
  geom_linerange(aes(xmin = ll, xmax = hh), position = position.offset) +
  geom_point(position = position.offset, color="black") + 
  geom_vline(xintercept = 0, linetype="dashed")
```

It becomes apparent that none of the variables are conditionally independent of each other: learning either variable does not diminish the information gained from the other variables. Consequently, we assume all three causal assumptions being correct.

#### Requirements Quality and Missing Entities

According to our DAG, we assume that $rq \rightarrow missing.associations$, $rq \rightarrow missing.entities$, and $missing.associations \rightarrow missing.associations$. We test whether the first and/or last hypothesis are true by comparing the selected model with other candidates where one of the two factors is missing. 

```{r comparison-formula-2}
f.no.rq <- associations.missing | trials(associations.expected) ~ 1 + 
  (1|PID) + 
  period +
  entities.missing +
  rel.duration + 
  exp.se.scaled + exp.re.scaled + edu + primary.role + 
  model.train + model.occ +
  dom.os + dom.db

f.no.me <- associations.missing | trials(associations.expected) ~ 1 + 
  (1|PID) + 
  RQ*period +
  rel.duration + 
  exp.se.scaled + exp.re.scaled + edu + primary.role + 
  model.train + model.occ +
  dom.os + dom.db
```

```{r model-skew-comp-2}
m.no.rq <-
  brm(data = d, family = binomial, f.no.rq, prior = c(
      prior(normal(-0.9, 0.1), class=Intercept),
      prior(normal(0, 0.1), class=b),
      prior(weibull(2, 0.5), class=sd)
    ),
    iter = 4000, warmup = 1000, chains = 4, cores = 4,
    seed = 4, 
  )

m.no.me <-
  brm(data = d, family = binomial, f.no.me, prior = priors,
    iter = 4000, warmup = 1000, chains = 4, cores = 4,
    seed = 4
  )
```

We compare the predictors of interest ($\beta_{exp.se\_scaled}$ and $\beta_{dom.os}$) to see whether the estimates change when including either of the variables.

```{r conditional-independencies-2}
param.m <- mcmc_intervals_data(m)
param.m.no.rq <- mcmc_intervals_data(m.no.rq)
param.m.no.me <- mcmc_intervals_data(m.no.me)

param.m$model <- "full model"
param.m.no.rq$model <- "no requirements quality"
param.m.no.me$model <- "no missing entity"

combined.estimates <- rbind(param.m, param.m.no.rq, param.m.no.me) %>% 
  filter(parameter %in% c("b_RQ1", "b_RQ2", "b_RQ3", "b_entities.missing"))

position.offset <- position_nudge(
  y = ifelse(
    combined.estimates$model == "full model",
    -0.1,
    ifelse(combined.estimates$model == "no dom", 0.1, 0)))

ggplot(data = combined.estimates,
       mapping = aes(x = m, y = parameter, color = model)) +
  geom_linerange(aes(xmin = l, xmax = h), position = position.offset, linewidth = 2) +
  geom_linerange(aes(xmin = ll, xmax = hh), position = position.offset) +
  geom_point(position = position.offset, color="black") + 
  geom_vline(xintercept = 0, linetype="dashed")
```

The parameter estimations in the full model move more towards the center but none of their influences is completely diminished, meaning that it none of the variables are conditionally independent from one another.

### Model selection

We visualize the summary of updated posterior distributions - which does not yet answer the initial question, but indicates the direction and strength that each predictor has on the response variable.

```{r model-summary}
summary(m)
```

### Model Evaluation

#### Posterior Predictions

We let the model predict the response variable with a fixed independent variable (RQ=1 (passive voice), RQ=2 (ambiguous pronoun), and RQ=3 (passive voice + ambiguous pronoun)) and representative values for the context factors. This posterior prediction takes all uncertainty, which the model picked up, into account.

```{r posterior-comparison-passive}
evaluate.model(model=m, treatment=1)
```

```{r posterior-comparison-pronoun}
evaluate.model(model=m, treatment=2)
```

```{r posterior-comparison-passive-pronoun}
evaluate.model(model=m, treatment=3)
```

These predictions show that the use of passive voice has a slight while the use of ambiguous pronouns has a strong impact on the number of missing associations in a domain model.

These values can also be obtained via `brms::conditional_effects`: by selecting the treatment `RQ` as the effect, the `posterior_epred` method predicts the distribution of the expected value (with a trial size fized to 1) when fixing the treatment to one of the four available factors while maintaining all other independent variables at representative values.

```{r posterior-comparison}
conditional_effects(m, effects="RQ", method="posterior_epred")
```

#### Marginal Effect

Additionally, we look at some marginal and interaction effects. Marginal effects represent the isolated effect of one variable on the outcome while all other effects are fixed at at a representative value. Interaction effects visualize the interaction between two predictors.

For example, the marginal impact of the different levels of domain knowledge look as follows:

```{r marginal-domainknowledge}
conditional_effects(m, effects="dom.os")
```

The marginal effect of the number of missing entities (`entities.missing`) shows the strong impact of missing entities on missing associations. Logically, an association must be either missing or wrong if at least one of its entities is missing. Including this predictor factors out this propagated effect.

```{r marginal-entities}
conditional_effects(m, effects="entities.missing")
```

The marginal effect of the relative duration shows no effect of duration on number of missing associations.

```{r marginal-duration}
conditional_effects(m, effects="rel.duration")
```

Next, we visualize the interaction effect between the period variable (i.e., the time slot in which a specific requirements specification was processed) and the treatment to identify any potential carryover effects.

```{r interaction-treatment-period}
conditional_effects(m, effects=c("period", "period:RQ"))
```

The period has a notably decreasing impact, hinting at a learning effect that is factored out of the other variables by including the period variable. The interaction shows that there is no carryover effect.
