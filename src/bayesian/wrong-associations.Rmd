---
title: "Requirements Quality Impact on Wrong Associations"
author: "Julian Frattini"
date: '2023-09-28'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

source("../util/setup.R")
source("../util/model-eval.R")

path_figures <- "../../figures/"
```

This document contains the investigation of the impact of **requirements quality** on the number of **wrong associations** in the process of domain modeling. A wrong association is an association where one of the connected nodes should actually not be involved while a third, not connected node, should be.

## Data Loading

```{r data}
source("../util/data-preprocessing.R")
d <- load.data()
```

## Bayesian Data Analysis 

We perform a Bayesian data analysis to answer the question: which factors influence the number of wrong associations in a domain model.

### Graphical Causal Model

We draw a Directed Acyclic Graph (DAG) to make our causal assumptions explicit.

```{r dag}
dag <- dagify(
  wrong ~ rq + dur + edu + exp + dom + task + tool,
  dom ~ exp,
  exposure = "rq",
  outcome = "wrong",
  labels = c(wrong = "wrong.associations", 
             dur = "duration",
             rq= "requirements.quality", 
             edu = "education", exp="experience",
             dom = "domain.knowledge", task = "task.experience",
             tool = "tool.experience"),
  coords = list(x=c(rq=1, wrong=2, dur=1.5, edu=1, exp=1, dom=1.2, task=1, tool=1),
                y=c(rq=-1, wrong=-1.5, dur=0, edu=-2, exp=-3.5, dom=-4, task=-2.5, tool=-3))
)

ggdag_status(dag, 
             use_labels = "label", 
             text = FALSE) +
  guides(fill = "none", color = "none") + 
  theme_dag()
```

### Formula Definition and Prior Selection

We derive our formula from our DAG to infer the strength of causal relationships from it.

#### Model Definition

Based on the DAG, we construct a model with the following parameters:

| Term | Predictor | Rationale |
|---|---|---|
| `1` | Intercept | General challenge of connecting entities wrongly |
| `(1|PID)` | Participant-specific variability | General ability of an individual participant with id `PID` to connect entities wrongly |
| `RQ*dom.os` | Interaction | Interaction effect between quality defects and domain knowledge, i.e., the mitigation of the defect's influence through domain knowledge |
| (`RQ`) |  Main Factor | Influence of a requirements quality defect on connecting entities wrongly |
| (`dom.os`) | Domain knowledge | Influence of the availability of domain knowledge |
| `RQ*period` | Carryover | Interaction effect between the treatment and the period, i.e., whether some treatments have a stronger effect in later periods |
| (`period`) | Sequence effect | Period in which the subject applies the treatment |
| `rel.duration` | Time | (Self-made) time pressure to complete the domain model |
| *rest* | Confounders | Context factors influencing the causal relationships. |

Predictors in brackets (e.g., (`RQ`)) are implied by the interaction effects (e.g., `RQ*dom.os`) and do not explicitly appear in the formula. The explicit interaction effect `RQ*dom.os` stems from the hypothesis that domain knowledge may compensate ambiguity in requirements specifications.

```{r formula}
formula <- associations.wrong | trials(associations.found) ~ 1 + 
  (1|PID) + 
  RQ*dom.os +
  RQ*period +
  rel.duration + 
  exp.se.scaled + exp.re.scaled + edu + primary.role + 
  model.train + model.occ
                    
#get_prior(formula, data=d, family=binomial)
```

For each predictor, we select an appropriate prior that reflects our prior belief about the causal relationship. We use weakly informative priors to allow the model in the later estimation step to infer the strength of the causal relations itself. The only exceptions are:

* Prior for the intercept: previous research shows that producing domain modeling is in general a not too complex task for engineers. Hence, we select a mean $\mu_{\alpha} < logit(0.5) = 0$ to represent that it is less likely than random to connect associations wrong.
* Prior for the treatment: the treatment of the prior is expected to have a greater influence than the less definitive context variables, hence $\sigma_{RQ} > \sigma$.

```{r priors}
priors <- c(
  prior(normal(-2.5, 1), class=Intercept),
  prior(normal(0, 1), class=b, coef="RQ1"),
  prior(normal(0, 1), class=b, coef="RQ2"),
  prior(normal(0, 1), class=b, coef="RQ3"),
  prior(normal(0, 0.5), class=b),
  prior(weibull(2, 1), class=sd)
)
```

#### Prior Evaluation

We sample directly from the priors (i.e., sample without the model updating its parameter distributions based on the observed data) to evaluate the feasibility of the priors.

```{r model-sample-priors}
m.prior <-
  brm(data = d, family = binomial, formula, prior = priors,
    iter = 4000, warmup = 1000, chains = 4, cores = 4,
    seed = 4, sample_prior="only",
    file = "fits/wrong.associations.prior"
  )
```

With the samples from the prior distributions, we conduct a prior predictive check to ensure that the priors are appropriate, i.e., the actual observations are realistic given the prior knowledge of the model.

```{r prior-predictive-check}
ndraws <- 100
priorpc <- brms::pp_check(m.prior, ndraws=ndraws, type="bars")
priorpc
```

The distribution of draws encompasses the actually observed data, meaning that the actual observations lie within the realm of belief defined by the priors. We accept our priors as feasible.

### Model Training

Now, we execute the estimation step by updating the prior distributions based on the observed data `d`.

```{r model}
m <-
  brm(data = d, family = binomial, formula, prior = priors,
    iter = 4000, warmup = 1000, chains = 4, cores = 4,
    seed = 4, 
    file = "fits/wrong.associations"
  )
```

We perform a posterior predictive check to ensure that the model has learned properly.

```{r posterior-predictive-check}
postpc <- brms::pp_check(m, type="bars", ndraws=ndraws)
postpc
```

The distribution of draws still encompasses the actually observed data. Additionally, the distribution grew narrower around the observed data, indicating that the posterior distributions more accurately reflect the the observed data. The following plot shows both prior and posterior predictions and compares their distribution against the actually observed data.

```{r vis-predictions}
# obtain the distribution of the prior predictions
d.priorpc <- priorpc$plot_env$yrep_data %>%
  rename(all_of(c(lo = "lo", est = "mid", hi = "hi"))) %>% 
  mutate(type = "Prior")

# obtain the distribution of the posterior predictions
d.postpc <- postpc$plot_env$yrep_data %>% 
  rename(all_of(c(lo = "lo", est = "mid", hi = "hi"))) %>% 
  mutate(type = "Posterior")

# combine the two data frames
d.predictions <- rbind(d.priorpc, d.postpc)

# obtain the actually observed distribution
d.actual <- d %>%
  group_by(associations.wrong) %>%
  summarize(actual = n()) %>%
  rename(x = "associations.wrong")

# plot both the actually observed data as well as the prior and posterior distribution
ggplot() + 
  geom_bar(data=d.actual, aes(x=x, y=actual), stat="identity") +
  geom_errorbar(data=d.predictions, aes(x=ifelse(type=="Prior", x-0.2, x+0.2), ymin = lo, ymax = hi, color=type), width=0.3) +
  geom_point(data=d.predictions, aes(x=ifelse(type=="Prior", x-0.2, x+0.2), y=est, color=type), size=3) +
  labs(x = "Number of wrong associations", y = "Occurrences", color = "Predictions") +
  guides(color = guide_legend(reverse=T))
```

```{r vis-predictions-save}
ggsave(filename=file.path(path_figures, "results/wrong-associations-predictive-checks.pdf"), width=7, height=5)
```

Because we only trained one model, there is no need to select the best performing model using leave-one-out comparison `loo_compare`.

### Comparison regarding conditional Independencies

We test for conditional independencies. According to our DAG, we assume that $exp.se \rightarrow wrong.associations$, $exp.se \rightarrow dom$, and $dom \rightarrow wrong.associations$. We test whether the first and/or last hypothesis are true by comparing the selected model with other candidates where one of the two factors is missing. 

```{r comparison-formula}
f.no.exp <- associations.wrong | trials(associations.found) ~ 1 + 
  (1|PID) + 
  RQ*dom.os +
  RQ*period +
  rel.duration + 
  exp.re.scaled + edu + primary.role + 
  model.train + model.occ

f.no.dom <- associations.wrong | trials(associations.found) ~ 1 + 
  (1|PID) + 
  RQ*period +
  rel.duration + 
  exp.se.scaled + exp.re.scaled + edu + primary.role + 
  model.train + model.occ
```

```{r model-skew-comp}
m.no.exp <-
  brm(data = d, family = binomial, f.no.exp, prior = priors,
    iter = 4000, warmup = 1000, chains = 4, cores = 4,
    seed = 4, 
  )

m.no.dom <-
  brm(data = d, family = binomial, f.no.dom, prior = priors,
    iter = 4000, warmup = 1000, chains = 4, cores = 4,
    seed = 4
  )
```

We compare the predictors of interest ($\beta_{exp.se\_scaled}$ and $\beta_{dom.os}$) to see whether the estimates change when including either of the variables.

```{r conditional-independencies}
param.m <- mcmc_intervals_data(m)
param.m.no.exp <- mcmc_intervals_data(m.no.exp)
param.m.no.dom <- mcmc_intervals_data(m.no.dom)

param.m$model <- "full model"
param.m.no.exp$model <- "no exp.se"
param.m.no.dom$model <- "no dom"

combined.estimates <- rbind(param.m, param.m.no.exp, param.m.no.dom) %>% 
  filter(parameter %in% c("b_exp.se.scaled", 
                          "b_dom.os.L", "b_dom.os.Q", "b_dom.os.C", "b_dom.os.E4", 
                          "b_dom.db.L", "b_dom.db.Q", "b_dom.db.C"))

position.offset <- position_nudge(
  y = ifelse(
    combined.estimates$model == "full model",
    -0.1,
    ifelse(combined.estimates$model == "no dom", 0.1, 0)))

ggplot(data = combined.estimates,
       mapping = aes(x = m, y = parameter, color = model)) +
  geom_linerange(aes(xmin = l, xmax = h), position = position.offset, linewidth = 2) +
  geom_linerange(aes(xmin = ll, xmax = hh), position = position.offset) +
  geom_point(position = position.offset, color="black") + 
  geom_vline(xintercept = 0, linetype="dashed")
```

It becomes apparent that none of the variables are conditionally independent of each other: learning either variable does not diminish the information gained from the other variables. Consequently, we assume all three causal assumptions being correct.

### Model selection

We visualize the summary of updated posterior distributions - which does not yet answer the initial question, but indicates the direction and strength that each predictor has on the response variable.

```{r model-summary}
summary(m)
```

These distributions can also be visualized graphically in addition to the Markov chains, which also provides diagnostics for the successful execution of the estimation step.

```{r model-diagnostics}
plot(m)
```

### Model Evaluation

#### Posterior Predictions

We let the model predict the response variable with a fixed independent variable (RQ=1 (passive voice), RQ=2 (ambiguous pronoun), and RQ=3 (passive voice + ambiguous pronoun)) and representative values for the context factors. This posterior prediction takes all uncertainty, which the model picked up, into account.

```{r posterior-comparison-passive}
evaluate.model(model=m, treatment=1)
```

```{r posterior-comparison-pronoun}
evaluate.model(model=m, treatment=2)
```

```{r posterior-comparison-passive-pronoun}
evaluate.model(model=m, treatment=3)
```

These values can also be obtained via `brms::conditional_effects`: by selecting the treatment `RQ` as the effect, the `posterior_epred` method predicts the distribution of the expected value (with a trials size of 1) when fixing the treatment to one of the four available factors while maintaining all other independent variables at representative values.

```{r posterior-comparison}
conditional_effects(m, effects="RQ", method="posterior_epred")
```

#### Marginal and Interaction Effects

Additionally, we look at some marginal and interaction effects. Marginal effects represent the isolated effect of one variable on the outcome while all other effects are fixed at at a representative value. Interaction effects visualize the interaction between two predictors.

Firstly, we investigate the effect of the relative duration to see whether the time taken to create a domain model has an effect on the response variable.

```{r marginal-duration}
conditional_effects(m, effects="rel.duration")
```

Time-pressure negatively affects the number of wrong associations. The less time a subject took for a domain model, the higher the likelihood of connecting associations wrongly.

Additionally, we investigate the marginal effect of one of the more significant context factors, the existence of prior training in modeling. The graph shows that there is a slight difference, but the overlapping confidence intervals undermine the significance of the effect.

```{r marginal-train}
conditional_effects(m, effects="model.train")
```

Next, we investigate the interaction between domain knowledge and the treatment. The hypothesis is that domain knowledge (in this case: particularly domain knowledge about open science/source) mitigates the effect of the treatments.

```{r interaction-treatment-domainknowledge}
conditional_effects(m, effects="dom.os:RQ")
```

The interaction effect shows a negative trend for `RQ=2` and `RQ=3`, meaning that the higher the domain knowledge, the smaller the impact of an ambiguous pronoun on the number of wrong associations. However, for `RQ=3`, the relationship is rather a bell-curve, meaning that 

Next, we visualize the interaction effect between the period variable (i.e., the time slot in which a specific requirements specification was processed) and the treatment to identify any potential carryover effects.

```{r interaction-treatment-period}
conditional_effects(m, effects=c("period", "period:RQ"))
```

The period has barely an effect on the response variable, such that a learning effect can be excluded. The interaction, however, shows that the effect of ambiguous pronouns was amplified in later periods.

#### Within-Subject Variance

Finally, we visualize the within-subject variance, i.e., the impact on the response variable that is caused by the particularities of the experiment participant itself. It can be assumed that this variance covers all context factors not explicitly included in the data.

```{r subject-variance}
r_fit <- m %>% 
  tidy() %>% 
  mutate(term = janitor::make_clean_names(term)) %>% 
  split(~term)

intercept <- r_fit$intercept$estimate

m %>% 
  linpred_draws(
    datagrid(
      PID = unique(d$PID), 
      RQ = unique(d$RQ), 
      model = m)) %>% 
  mutate(offset = intercept - .linpred) %>% 
  ungroup() %>% 
  mutate(PID = fct_reorder(factor(PID), offset, .fun=mean)) %>% 
  ggplot(aes(x = offset, y = PID)) +
    geom_vline(xintercept = 0) + 
    stat_pointinterval()
```