---
title: "Requirements Quality Impact on Superfluous Entities"
author: "Julian Frattini"
date: '2023-04-13'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

source("../util/setup.R")
source("../util/model-eval.R")
```

This document contains the investigation of the impact of **requirements quality** on the number of **superfluous entities** in the process of domain modeling. An entity is superfluous if it is not implied by the requirements specification and, hence, only restricts the solution space.

## Data Loading

```{r data}
source("../util/data-preprocessing.R")
d <- load.data()

ndraws <- 100
```

## Bayesian Data Analysis 

We perform a Bayesian data analysis to answer the question: which factors influence the number of wrong associations in a domain model.

### Formula Definition and Prior Selection

We derive our formula from our DAG to infer the strength of causal relationships from it.

#### Models

Based on the DAG, we construct a model with the following parameters:

| Term | Predictor | Rationale |
|---|---|---|
| `1` | Intercept | General likelihood of adding superfluous entities |
| `(1|PID)` | Participant-specific variability | General ability of an individual participant with id `PID` to add a superfluous entity to the domain model |
| `RQ*period` | Carryover | Interaction effect between the treatment and the period, i.e., whether some treatments have a stronger effect in later periods |
| (`RQ`) |  Main Factor | Influence of a requirements quality defect |
| (`period`) | Sequence effect | Period in which the subject applies the treatment |
| `rel.duration` | Time | (Self-made) time pressure to complete the domain model |
| *rest* | Confounders | Context factors influencing the causal relationships. |

Predictors in brackets (e.g., (`RQ`)) are implied by the interaction effects (e.g., `RQ*dom.os`) and do not explicitly appear in the formula.

```{r formula}
formula <- entities.superfluous ~ 1 + 
  (1|PID) + 
  RQ*period +
  rel.duration + 
  exp.se.scaled + exp.re.scaled + edu + primary.role + 
  dom.db + dom.os +
  model.train + model.occ

#get_prior(formula, data=d, family=zero_inflated_poisson)
```

We select uninformative priors for the predictors in the formula:

```{r priors-zip}
priors.zip <- c(
  prior(normal(0, 1), class=Intercept),
  prior(normal(0, 0.3), class=b),
  prior(weibull(2, 1.5), class=sd),
  prior(beta(2, 4), class=zi)
)
```

```{r priors-zinb}
priors.zinb <- c(
    prior(normal(0, 1), class=Intercept),
    prior(normal(0, 0.2), class=b),
    prior(weibull(2, 1.5), class=sd),
    prior(gamma(2, 2), class=shape),
    prior(beta(2, 4), class=zi)
  )
```


#### Prior Predictive Checks

```{r model-priors-zip}
m.prior.zip <-
  brm(data = d, family = zero_inflated_poisson, formula, prior = priors.zip,
    iter = 4000, warmup = 1000, chains = 4, cores = 4,
    seed = 4, sample_prior="only",
    file = "fits/superfluous.entities.zip.prior"
  )
```

```{r model-priors-zip}
m.prior.zinb <-
  brm(data = d, family = zero_inflated_negbinomial, formula, prior = priors.zinb,
    iter = 4000, warmup = 1000, chains = 4, cores = 4,
    seed = 4, sample_prior="only",
    file = "fits/superfluous.entities.zinb.prior"
  )
```

```{r prior-predictive-check}

priorpc.zip <- brms::pp_check(m.prior.zip, ndraws=ndraws, type="bars")
priorpc.zinb <- brms::pp_check(m.prior.zinb, ndraws=ndraws, type="bars")

priorpc.zip / priorpc.zinb
```

### Model Training

Now, we execute the estimation step by updating the prior distributions based on the observed data `d`.

```{r model-zip}
m.zip <-
  brm(data = d, family = zero_inflated_poisson, formula, prior = priors.zip,
    iter = 4000, warmup = 1000, chains = 4, cores = 4,
    seed = 4, 
    file = "fits/superfluous.entities.zip"
  )
```

```{r model-zinb}
m.zinb <-
  brm(data = d, family = zero_inflated_negbinomial, formula, prior = priors.zinb,
    iter = 4000, warmup = 1000, chains = 4, cores = 4,
    seed = 4, 
    file = "fits/superfluous.entities.zinb"
  )
```

We perform a posterior predictive check to ensure that the model has learned properly.

```{r posterior-predictive-check}
postpc.zip <- brms::pp_check(m.zip, ndraws=ndraws, type="bars")
postpc.zinb <- brms::pp_check(m.zinb, ndraws=ndraws, type="bars")

postpc.zip / postpc.zinb
```

The distribution of draws is not yet close enough to the real data.

### Model Comparison

We compare the predictive capability of the two models using the leave-one-out comparison.

```{r loo-compare}
m.zip <- add_criterion(m.zip, criterion = "loo")
m.zinb <- add_criterion(m.zinb, criterion = "loo")

loo_compare(m.zip, m.zinb)
```

According to loo-compare, there is no significant difference in the predictive power of the two models. We can select and proceed either of the two models.

```{r model-selection}
m <- m.zip
```

### Model Summary

We visualize the summary of updated posterior distributions - which does not yet answer the initial question, but indicates the direction and strength that each predictor has on the response variable.

```{r model-summary}
summary(m)
```

### Model Evaluation

#### Posterior Predictions

We let the model predict the response variable with a fixed independent variable (RQ=1 (passive voice), RQ=2 (ambiguous pronoun), and RQ=3 (passive voice + ambiguous pronoun)) and representative values for the context factors. This posterior prediction takes all uncertainty, which the model picked up, into account.

```{r posterior-comparison-passive}
evaluate.model(model=m, treatment=1)
```

```{r posterior-comparison-pronoun}
evaluate.model(model=m, treatment=2)
```

```{r posterior-comparison-passive-pronoun}
evaluate.model(model=m, treatment=3)
```

The posterior predictions indicate that pure ambiguous pronouns (not in conjunction with passive voice) benefit superfluous entities.

These values can also be obtained via `brms::conditional_effects`: by selecting the treatment `RQ` as the effect, the `posterior_epred` method predicts the distribution of the expected value when fixing the treatment to one of the four available factors while maintaining all other independent variables at representative values.

```{r posterior-comparison}
conditional_effects(m, effects="RQ", method="posterior_epred")
```

#### Marginal Effect

Additionally, we look at some marginal and interaction effects. Marginal effects represent the isolated effect of one variable on the outcome while all other effects are fixed at at a representative value. Interaction effects visualize the interaction between two predictors.

we visualize the interaction effect between the period variable (i.e., the time slot in which a specific requirements specification was processed) and the treatment to identify any potential carryover effects. The learning effect is minimal and the carryover effect of all treatments largely negligible. Only the impact of pure ambiguous pronouns (`RQ=2`) scales with the period variable.

```{r interaction-treatment-period}
conditional_effects(m, effects=c("period", "period:RQ"))
```

Visualizing the marginal effect of the relative duration shows that the longer a participant took the more likely they were to introduce superfluous entities.

```{r marginal-effects-duration}
conditional_effects(m, effects="rel.duration")
```

The marginal effect of modeling occurance (i.e., how often a participant uses modeling techniques in their job) shows a slight effect of the middle frequencies. This hints at the fact that both (1) experienced modelers (`model.occ=often`) know what they are doing and (2) novel modelers (`model.occ=none`) paid closer attention to the task.

```{r marginal-effects-modeling}
conditional_effects(m, effects="model.occ")
```

