---
title: "Inter-rater Agreement"
author: "Julian Frattini"
date: '2023-10-10'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# data wrangling and manipulation
library(tidyverse)
library(xlsx)
library(patchwork)
source("../util/data-preprocessing.R")

library(lsa)
```

In this document, we calculate the inter-rater agreement of the labeling task, where two independent raters annotated the issues they found in the submitted domain models. There are six types of isseus:

* Missing entity: An entity in the ground truth has no semantic equivalent in the submitted domain model.
* Too-coarse entity: An entity in the submitted domain model represents two or more entities in the ground truth.
* Superfluous entity: An entity in the submitted domain model has no semantical equivalent in the ground truth.
* Missing association: An association in the ground truth has no semantic equivalent in the submitted domain model.
* Miswired association: An association in the ground truth has a semantic equivalent in the submitted domain model, but either the target or the source of the association is incorrect.
* Superfluous association: An association in the submitted domain model has no semantic equivalent in the ground truth.

```{r response-variables}
response.vars <- c("Missing.entity", "Too.coarse.entity", "Superfluous.entity", "Missing.association", "Miswired.association", "Superfluous.association")
```

## Data Loading

First, load the data and cast relevant columns to their respective data type

```{r data-loading}
r1 <- read.xlsx(file="../../data/raw/rqi-results.xlsx", sheetName="Statistics") %>% 
  mutate(PID = as.numeric(PID))

r2 <- read.xlsx(file="../../data/raw/rqi-results-overlap.xlsx", sheetName="Statistics") %>% 
  mutate(PID = as.numeric(PID))
```

Then, determine the participants' IDs (PID) of those, where the submissions were annotated by both raters.

```{r overlap}
pid1 <- r1 %>% distinct(PID)
pid2 <- r2 %>% distinct(PID)

overlap <- intersect(pid1, pid2)
```

Filter the first data frame to only contain the observations of interest.

```{r filter}
r1 <- r1 %>% filter(PID %in% overlap$PID)
#r2 <- r2 %>% filter(PID %in% overlap$PID)
```

## Inter-rater Agreement Calculation

Our response variables are numeric, i.e., we only care about the *number* of issues identified in each requirement. Every requirement is hence characterized by an evaluation on six dimensions (one for each response variable). The appropriate measure of agreement is, hence, cosine similarity, which we calculate for each observation.

```{r cosine-similarity}
cosine.similarity <- c()

for (i in 1:nrow(r1)) {
  vec1 <- r1[i,] %>% select(all_of(response.vars))
  vec2 <- r2[i,] %>% select(all_of(response.vars))
  
  sim <- cosine(as.numeric(as.vector(vec1)), as.numeric(as.vector(vec2)))
  
  cosine.similarity <- c(cosine.similarity, sim)
}
```

Finally, calculate the mean of the 12 cosine similarities to determine the average rating agreement.

```{r mean-agreement}
mean(cosine.similarity)
```

